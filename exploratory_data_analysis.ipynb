{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4f5eab-9552-4091-af3d-9124427dd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154b30f8-32b8-40ee-9aba-a3a0819ea81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>Fears for T N pension after talks</th>\n",
       "      <th>Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Open Letter Against British Copyright Indoctri...</td>\n",
       "      <td>The British Department for Education and Skill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   3                  Fears for T N pension after talks  \\\n",
       "0  4  The Race is On: Second Private Team Sets Launc...   \n",
       "1  4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "2  4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "3  4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "4  4  Open Letter Against British Copyright Indoctri...   \n",
       "\n",
       "  Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.  \n",
       "0  SPACE.com - TORONTO, Canada -- A second\\team o...                                                                               \n",
       "1  AP - A company founded by a chemistry research...                                                                               \n",
       "2  AP - It's barely dawn when Mike Fitzpatrick st...                                                                               \n",
       "3  AP - Southern California's smog-fighting agenc...                                                                               \n",
       "4  The British Department for Education and Skill...                                                                               "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=r'C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\data\\test.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()  # Check column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff976715-c434-4aac-af79-dc551cc37313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7599 entries, 0 to 7598\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                                                                                                           Non-Null Count  Dtype \n",
      "---  ------                                                                                                                           --------------  ----- \n",
      " 0   3                                                                                                                                7599 non-null   int64 \n",
      " 1   Fears for T N pension after talks                                                                                                7599 non-null   object\n",
      " 2   Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.  7599 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 178.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353e2cef-5c29-45b4-a933-b2bde0497b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3', 'Fears for T N pension after talks',\n",
       "       'Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b23f51a5-01a1-4cab-88bb-94521e7f9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, names=[\"class_index\", \"title\", \"description\"], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16b214e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load English NLP model (for tokenization & stopword removal)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trisha\\ProjectEnvs\\myenv\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trisha\\ProjectEnvs\\myenv\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load English NLP model (for tokenization & stopword removal)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "980e201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e53c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English NLP model (for tokenization & stopword removal)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32064778-f507-4b9f-82d1-d1b475c964ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Preprocess text by removing special characters and stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove punctuation/numbers\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]  # Lemmatize & remove stopwords\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1edf6b-ded3-43c8-9ca6-a4ec0556298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # Combine title and description\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1ac7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "EMBEDDING_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\embeddings\\conceptnet_numberbatch_vectors.npy\"\n",
    "WORDS_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\embeddings\\conceptnet_numberbatch_words.npy\"\n",
    "TRAIN_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\data\\train.csv\"\n",
    "TEST_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\data\\test.csv\"\n",
    "SAVE_TRAIN_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\data\\train.pkl\"\n",
    "SAVE_TEST_PATH = r\"C:\\Users\\Trisha\\PycharmProjects\\NewsArticleClassification\\data\\test.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "060fbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"Load ConceptNet Numberbatch embeddings.\"\"\"\n",
    "    words = np.load(WORDS_PATH, allow_pickle=True)\n",
    "    vectors = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "    embeddings = {word: vectors[i] for i, word in enumerate(words)}\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d1e64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, embeddings, dim=300):\n",
    "    \"\"\"Convert text to a vector by averaging word embeddings.\"\"\"\n",
    "    words = text.split()\n",
    "    vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)  # Return zero vector if no words found\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73e4242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(filepath, embeddings):\n",
    "    \"\"\"Load, clean, and vectorize dataset.\"\"\"\n",
    "    df = pd.read_csv(filepath, names=[\"class_index\", \"title\", \"description\"], header=None)\n",
    "    df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]  # Merge title and description\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "    df[\"vector\"] = df[\"clean_text\"].apply(lambda x: text_to_vector(x, embeddings))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4036ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Processing training data...\n",
      "Processing test data...\n",
      "Preprocessing completed! Data saved.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading embeddings...\")\n",
    "    embeddings = load_embeddings()\n",
    "\n",
    "    print(\"Processing training data...\")\n",
    "    train_data = process_dataset(TRAIN_PATH, embeddings)\n",
    "    with open(SAVE_TRAIN_PATH, \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    print(\"Processing test data...\")\n",
    "    test_data = process_dataset(TEST_PATH, embeddings)\n",
    "    with open(SAVE_TEST_PATH, \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n",
    "\n",
    "    print(\"Preprocessing completed! Data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
